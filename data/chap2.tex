\chapter{相关的工作}

\section{时空正则化}
传统的正则化主要都是针对数据在空间上的特性来进行考虑的，另一方面，有不少研究方向则主要集中在
发现数据随着时间变化的模式这个问题上（例如时间文本挖掘 \cite{mei2005det}）。真正把时间放在正则化
的上下文来考虑并和传统的空间正则化结合起来，也是最近才逐渐有相关的研究出现。

其中进化聚类（Evolutionary clustering）\cite{chakrabarti2006ec,chi2007esc} 与我们在这里考虑的
时空正则化的思想最为接近。进化聚类就是将考虑了时间正则化的
聚类算法。在 \cite{chi2007esc} 中，Yun 提出了两个用于引进时间平滑度到 K-means 和谱聚类中的框架：

\begin{itemize}
  \item \textbf{Preserving Cluster Quality (PCQ)}：在这个框架中，在时间 t 时得到的聚类分割被直接
      用于对时间 t-1 时的数据进行分割，如果在时间 t 时有几个聚类方式都很好，那么在 t-1 时的数据
      上表现最好的那个将胜出。具体地说，如下的一个正则化项被加到 K-means 的损失函数上：
\begin{align*}
\text{Cost}_{KM}&=\alpha \cdot CS_{KM}+\beta\cdot CT_{KM} \\
&=\alpha \cdot KM_t|_{Z_t} + \beta \cdot KM_{t-1}|_{Z_t} \\
&=\alpha\cdot\sum_{l=1}^k\sum_{i\in\mathcal{V}_{l,t}}\|\vec{v}_{i,t}-\vec{\mu}_{l,t}\|^2
\\
&\mathrel{\phantom{=}}{}+\beta\cdot\sum_{l=1}^k\sum_{i\in\mathcal{V}_{l,t}}\|\vec{v}_{i,t-1}-\vec{\mu}_{l,t-1}\|^2
\end{align*}
      同样的方法可以用在谱聚类上，得到的结果可以化归到标准的谱聚类的形式并通过迭代的方法进行求解。
      他们还证明了进化 K 平均聚类算法是进化谱聚类算法的一个特殊情况，并可以用类似的办法进行求解。
  \item \textbf{Preserving Clustering Membership (PCM)}：在这个框架中，时间 t 时的聚类分割方式
      被直接与时间 t-1 时的分割方式进行比较，两个分割的比较基于 $\chi$ 方统计分布 \cite{hubert1985cp}：
\[
\chi^2(Z_t,Z_{t-1})=n\left(\sum_{i=1}^k\sum_{j=1}^k\frac{|\mathcal{V}_{ij}|^2}{|\mathcal{V}_{i,t}|
\cdot |\mathcal{V}_{j,t-1}|}-1\right)
\]
      同 PCQ 类似，一个额外的正则化项被添加到原来的损失函数，并化归到合适的形式以便进行迭代求解。
\end{itemize}

\section{视频压缩}
在这一节中，我们简要地介绍 Cheng 的视频压缩方法
\cite{learning-to-compress-images}
，他们的工作首先考虑了使用机器学习的方法来进行视频压缩，我们的改进算法使用了类似的思路。
从机器学习的视觉来看，视频着色可以被理解为一个半监督学习
的过程。给定一个视频帧的一些有颜色的像素点（有标签的数据）和一些灰度像素点（无标签的数据），我们
希望学习一个模型，用于在灰度信息的基础上预测当前以及后续一些帧的颜色值（标签）。

接下来我们将介绍用于半监督学习的拉普拉斯正则化的最小二乘法（LapRLS）。使用
LapRLS
来进行着色是基于这样的一个假设：如果两个点有类似的灰度值并且在空间上比较接近，那么他们的颜色值也很
有可能是非常相似的。我们用$\textbf{z} \in \mathbb{R}^d$
来表示有标签的数据点，$\textbf{x} \in \mathbb{R}^d$
来表示任意数据点（有标签或者没有标签的）。

考虑这样一个线性回归模型$y=\textbf{a}^T \textbf{x} +
\epsilon$，其中$y$是因变量，$\textbf{x}$是自变量，$\textbf{a}$是权重向量，而$\epsilon$是一个
位置的期望为零的误差。不同的观察值有不同的误差，这些误差之间相互独立，但是方差都等于$\sigma^2$。在
给定输入向量$\textbf{x}$和权重向量$\textbf{a}$的前提下，我们定义
$f(\textbf{x})=\textbf{a}^T\textbf{x}$为模型的输出。

LapRLS算法同时使用有标签的数据和无标签的数据来学习回归模型$f$。假设一共有$m$个点，其中$k$个有
标签。令$S$为相似度矩阵，LapRLS算法对如下优化问题进行求解：

\begin{eqnarray}
&J_{LapRLS}(\textbf{a}) =\sum_{i=1}^k \big( f(\textbf{z}_i) - y_i \big)^2 \nonumber \\
& + \frac{\lambda_1}{2} \sum_{i,j=1}^{m} \big( f(\textbf{x}_i) -
f(\textbf{x}_j)\big)^2 S_{ij} + \lambda_2 \|\textbf{a}\|^2
\label{eqn:LPP-least-square-error}
\end{eqnarray}

其中$y_i$是数据点$z_i$的标签。在我们选取的对称权重$S_{ij}$（$S_{ij}=S_{ji}$）下，给定的损失函数
将不允许相邻的点$\textbf{x}_i$和$\textbf{x}_j$被影射到互相远离的地方。因此，最小化
$J_{LapRLS}(\textbf{a})$就可以保证如果$\textbf{x}_i$和$\textbf{x}_j$相互接近的话，$f(\textbf{x}_i)$
和$f(\textbf{x}_j)$之间也会比较接近。对于相似度矩阵$S$有许多不同的选择，一个简单的定义如下所示：

\begin{equation}
S_{ij}=\left\{%
\begin{array}{ll}
    1, &
    \hbox{如果$\textbf{x}_i$在$\textbf{x}_j$的$p$个最接近的邻居中，}\\
    & \hbox{或者$\textbf{x}_j$在$\textbf{x}_i$的$p$个最接近的邻居中;} \\
    0, & \hbox{其他情况.} \\
\end{array}%
\right. \label{eqn:similarity}
\end{equation}

令$D$为一个对角阵，$D_{ii}=\sum_j
S_{ij}$并且$L=D-S$。矩阵$L$在谱图理论中被称为拉普拉斯算子。令$Z=(\textbf{z}_1,
\cdots, \textbf{z}_k)$, $X=(\textbf{x}_1, \cdots,
\textbf{x}_m)$，$\textbf{y}=(y_1, \cdots,
y_k)^T$，则最小化问题\ref{eqn:LPP-least-square-error}的解由下式给出：

\begin{equation}\label{eqn:LapRLS-solution}
\widehat{\textbf{a}}=\big(ZZ^T + \lambda_1 XLX^T +\lambda_2 I
\big)^{-1} Z\textbf{y}
\end{equation}

其中$I$是一个$d \times
d$的单位矩阵。LapRLS也可以在再生核希尔伯特空间（Reproducing Kernel
Hilbert Space,
RKHS）中进行，这将到处一个非线性的解。关于LapRLS的更多细节请参见\cite{Manifold-Regularization-Journal}。

