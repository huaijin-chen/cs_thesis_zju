\chapter{基于半监督学习的着色}
\label{chap:semi-supervised-learning}

在本章中，我们将介绍一种全新的使用半监督学习进行着色的方法。这个方法的基本假设是：每个像素点的颜色值可以由
在时空上邻近并且灰度值相似的像素点的颜色值重新构造出来。这个算法灵感来源于一些前人的工作
\cite{Manifold-Regularization-Journal,LLE,NPE}.

\section{算法}

我们用位置和灰度值来表示图像中的每一个像素。这样，每个像素可以由一个向量$\textbf{x}
\in
\mathbb{R}^d$来表示。和LapRLS类似，我们在像素点$\textbf{x}_1,
\cdots,
\textbf{x}_m$上构造一个邻接图$G$。为了获取这些点之间的关系，我们对如下的图上的正则化损失函数进行最小化：

\begin{equation}\label{eqn:regularized-regression}
    \mathcal{L}(f)= \sum_{i=1}^k \big( y_i - f(\mathbf{x}_i) \big)^2 + \lambda J(f),
\end{equation}

其中$J(f)$控制了假设空间的学习复杂度，系数$\lambda$控制了实验误差和模型复杂度之间的平衡性。

对于普通的图片，通常一个像素点的位置和灰度值都可以根据其相邻的像素估计出来。因此，我们将每个像素用其邻接
点线性表示出来，并使用这些线性系数来表征数据点之间的关系。重新构造所产生的误差由如下的损失函数来进行度量
\cite{LLE}：

$$
\phi (W) = \sum_{i=1}^{m} \| \mathbf{x}_i - \sum_{j=1}^{m} W_{ij}
\mathbf{x}_j \|^2,
$$

这个损失函数将每个数据点到它重新构造出来的值之间的距离的平方累加起来。需要注意的是，如果$\textbf{x}_i$和
$\textbf{x}_j$之间没有连接起来，$W_{ij}$将会等于零。这个代价函数最初是在\textit{局部线性嵌入}算法中用于
学习数据空间的几何结构的，详见\cite{LLE}。

现在考虑估计每个像素点的标签（即颜色值）的问题，我们希望每个像素点的标签也可以经过系数$W_{ij}$由与其相邻
的像素点的标签组合而来。我们认为如下这样的正则化项是合理的，可以用于选取一个较好的模型：

\begin{equation}\label{eqn:regularizer}
    J(f)=\sum_{i=1}^{m} \Big( f(\mathbf{x}_i) - \sum_{j=1}^{m} W_{ij} f(\mathbf{x}_j) \Big)^2.
\end{equation}

需要注意的是，在(\ref{eqn:regularizer})中，权值$W_{ij}$是固定的。对于线性函数$f(\textbf{x})=\textbf{a}^T
\textbf{x}$，(\ref{eqn:regularizer}将变成

\begin{equation}\label{eqn:linear-regularizer}
    J(\mathbf{a})=\sum_{i=1}^{m} \Big( \mathbf{a}^T \mathbf{x}_i -
    \sum_{j=1}^{m} W_{ij} \mathbf{a}^T \mathbf{x}_j \Big)^2.
\end{equation}

通过一些简单的代数变换，我们可以得到：

\begin{eqnarray*}
J(\mathbf{a})&=&\textbf{a}^T \Big( \sum_{i=1}^m \big( \mathbf{x}_i -
\sum_{j=1}^{m} W_{ij} \mathbf{x}_j\big) \big( \mathbf{x}_i -
\sum_{j=1}^{m} W_{ij} \mathbf{x}_j \big)^T \Big) \textbf{a}\\
&=&\textbf{a}^T \big(X-XW^T \big) \big(X-XW^T \big)^T
\textbf{a}\\
   &=& \mathbf{a}^T X(I-W)^T(I-W)X^T \mathbf{a}\\
   &\doteq& \mathbf{a}^T XMX^T \mathbf{a},
\end{eqnarray*}

其中$M=(I_W)^T(I_W)$。易知$M$是对称且半正定的的。令$y_i=f(\mathbf{x}_i)$，$\mathbf{y}
= (y_1, y_2, \cdots, y_k)^T$，则图像的最小平方误差如下：

\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
&& \sum_{i=1}^l \Big( y_i - \mathbf{a}^T \mathbf{x}_i \Big)^2\\
&=& \Big( \mathbf{y}-Z^T \mathbf{a}\Big)^T \Big( \mathbf{y}-Z^T \mathbf{a}\Big) \\
&=& \mathbf{y}^T \mathbf{y}-2\mathbf{a}^T Z\mathbf{y}+\mathbf{a}^T
ZZ^T \mathbf{a}.
\end{eqnarray*}

注意到$\mathbf{y}^T
\mathbf{y}$是一个常量，因此损失函数(\ref{eqn:regularized-regression})可以被归约为如下
形式：

\begin{equation}\label{eqn:final-loss-function}
    \mathcal{L}(\mathbf{a})= -2\mathbf{a}^T Z\mathbf{y}+\mathbf{a}^T ZZ^T \mathbf{a}
    + \lambda \mathbf{a}^T XMX^T \mathbf{a}.
\end{equation}

零$\mathcal{L}$的导数等于零，我们有：

\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
&& \frac{\partial \mathcal{L}}{\partial \mathbf{a}} =0 \nonumber \\
 &\Rightarrow& -2Z\mathbf{y}+2ZZ^T \mathbf{a}
    + 2\lambda XMX^T \mathbf{a}=0 \nonumber \\
 &\Rightarrow& \mathbf{a}=\Big( ZZ^T + \lambda XMX^T \Big)^{-1}Z\mathbf{y}
 \label{eqn:sol}
\end{eqnarray}

有时候矩阵$ZZ^T+\lambda
XMX^T$可能是奇异的，我们可以添加一个脊正则化项\cite{statistical-leraning}从而得到如下形式：
\begin{equation}
\label{eqn:regularized-regression-linear}
    \mathcal{L}(\textbf{a})= \sum_{i=1}^k \big( y_i - \textbf{a}^T \textbf{x}_i \big)^2 +
    \lambda_1 J(f) + \lambda_2 \|\textbf{a}\|^2,
\end{equation}
新的损失函数的解如下所示：
\begin{equation}
\label{eqn:linear-sol} \mathbf{a}=\Big( ZZ^T + \lambda_1 XMX^T +
\lambda_2 I \Big)^{-1}Z\mathbf{y}
\end{equation}
易知矩阵$ZZ^T + \lambda XMX^T$是半正定的。因此，矩阵$ZZ^T +
\lambda_1 XMX^T + \lambda_2 I$是可逆的。一旦得到了回归函数$f$，任意
像素点$\mathbf{x}$的标签就可以通过$f(\mathbf{x}) = \mathbf{a}^T
\mathbf{x}$估计出来。

\section{非线性推广}
\label{sec:nonlinear-generalization}
在本小节中，我们会讨论使用核方法\cite{Learning-with-Kernels}
来将正则化的回归算法推广到非线性的情况。

我们在关联到一个核函数$\mathcal{K}: \mathbb{R}^d\times \mathbb{R}^d
\rightarrow \mathbb{R}$再生核希尔伯特空间$\mathcal{H}$里来考虑这个
问题。在$\mathcal{H}$里的损失函数可以写成如下形式：
\begin{equation}
\label{eqn:rkhs-obj}
 \mathcal{L}(f)= \sum_{i=1}^k \big( y_i - f(\mathbf{x}_i) \big)^2 +
 \lambda_1 J(f) + \lambda_2 \|f\|_{\mathcal{H}}, f \in \mathcal{H}.
\end{equation}
由representer theorem
\cite{Learning-with-Kernels,Manifold-Regularization-Journal}可知，最
优的$f$具有如下形式：
\begin{equation}
\label{eqn:representer-theorem} f(\textbf{x})=\sum_{i=1}^m \alpha_i
\mathcal{K}(\textbf{x}, \textbf{x}_i)
\end{equation}
令$K_{XX}$为一个$m\times m$的核矩阵，其中$K_{XX,ij}=\mathcal{K}
(\textbf{x}_i, \textbf{x}_j)$，并令$K_{ZX}$为一个$k \times m$的核
矩阵，其中$K_{ZX,ij}=\mathcal{K}(\mathbf{z}_i\mathbf{x}_j)$。我们定义
$$
\textbf{f}_Z=\big( f(\textbf{z}_1), \cdots, f(\textbf{z}_k) \big)^T.
$$
显然$\textbf{f}_Z = K_{ZX} \pmb{\alpha}$，因此
\begin{eqnarray*}
&&\sum_{i=1}^k \big( y_i - f(\textbf{z}_i) \big)^2 = \big(
\textbf{y}
- \textbf{f}_Z \big)^T \big( \textbf{y} - \textbf{f}_Z \big)\\
&=&\big( \textbf{y} - K_{ZX} \pmb{\alpha} \big)^T \big( \textbf{y} -
K_{ZX} \pmb{\alpha} \big)\\
&=&\textbf{y}^T \textbf{y} - 2 \textbf{y}^T K_{ZX} \pmb{\alpha} +
\pmb{\alpha}^T K_{ZX}^T K_{ZX} \pmb{\alpha}
\end{eqnarray*}
类似地，我们定义
$$
\textbf{f}_X=\big( f(\textbf{x}_1), \cdots, f(\textbf{x}_m) \big)^T
= K_{XX} \pmb{\alpha}.
$$
则
\begin{eqnarray*}
J(f)&=&\sum_{i=1}^{m} \Big( f(\mathbf{x}_i) - \sum_{j=1}^{m} W_{ij}
f(\mathbf{x}_j) \Big)^2 \\
&=& \big( K_{XX} \pmb{\alpha} - W K_{XX} \pmb{\alpha} \big)^T \big(
K_{XX} \pmb{\alpha} - W K_{XX} \pmb{\alpha} \big) \\
&=& \pmb{\alpha}^T K_{XX} \big(I-W)^T(I-W) K_{XX} \pmb{\alpha}\\
&=& \pmb{\alpha}^T K_{XX} M K_{XX} \pmb{\alpha}
\end{eqnarray*}
并且
\begin{eqnarray*}
\|f\|^2_{\mathcal{H}}&=&\langle f, f \rangle \\
&=& \langle \sum_{i=1}^m \alpha_i \mathcal{K}(\cdot, \textbf{x}_i),
\sum_{j=1}^m \alpha_j \mathcal{K}(\cdot,
\textbf{x}_j) \rangle \\
&=& \sum_{i,j} \alpha_i \alpha_j \langle \mathcal{K}(\cdot,
\textbf{x}_i), \mathcal{K}(\cdot, \textbf{x}_j) \rangle \\
&=& \sum_{i,j} \alpha_i \alpha_j \mathcal{K} (\textbf{x}_i,
\textbf{x}_j)\\
&=& \pmb{\alpha}^T K_{XX} \pmb{\alpha}
\end{eqnarray*}
这样，损失函数可以被化简为
\begin{eqnarray*}
\mathcal{L}(f)&=&\textbf{y}^T \textbf{y} - 2 \textbf{y}^T K_{ZX}
\pmb{\alpha} + \pmb{\alpha}^T K_{ZX}^T K_{ZX} \pmb{\alpha}\\
&& + \lambda_1 \pmb{\alpha}^T K_{XX} M K_{XX} \pmb{\alpha} +
\lambda_2 \pmb{\alpha}^T K_{XX} \pmb{\alpha}
\end{eqnarray*}
令$\mathcal{L}$的导数等于零，则得到如下形式的解：
\begin{equation}
\label{eqn:kernel-regression-sol} \pmb{\alpha}=\big( K_{ZX}^T K_{ZX}
+ \lambda_1 K_{XX}M K_{XX} + \lambda_2 K_{XX} \big)^{-1} K_{ZX}^T
\textbf{y}
\end{equation}

